{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84fe4e9-eadd-4630-be84-f1a6aac0c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import vak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c42090-a865-43ea-a577-e93d0ea87767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ildefonso/Documents/repos/vocalpy/2023-messy-experiments\n"
     ]
    }
   ],
   "source": [
    "cd /home/ildefonso/Documents/repos/vocalpy/2023-messy-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efee260-7dd7-4527-9bc0-8f6749e38e73",
   "metadata": {},
   "source": [
    "First we make a constant representing the path to the root of the original dataset, and from there we get the two csv files that contain the train and test splits.\n",
    "\n",
    "We load those csv files into `pandas.DataFrame`s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5492b-31f1-4a1d-a7e8-de9bd24232f0",
   "metadata": {},
   "source": [
    "## 1. Prep source files\n",
    "\n",
    "After doing this once you should be able to re-load the csv for step 2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5ac8ff-292f-4496-96c7-fe6973268b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crowsetta\n",
    "\n",
    "\n",
    "TIMIT_TRANSCRIBER = crowsetta.Transcriber(format='timit')\n",
    "\n",
    "\n",
    "def simpleseq_from_timit_phn_path(\n",
    "    annot_path: pathlib.Path\n",
    ") -> crowsetta.formats.seq.SimpleSeq:\n",
    "    a_timit = TIMIT_TRANSCRIBER.from_file(annot_path)\n",
    "    seq = a_timit.to_seq()\n",
    "    a_simpleseq = crowsetta.formats.seq.SimpleSeq(\n",
    "        onsets_s=seq.onsets_s,\n",
    "        offsets_s=seq.offsets_s,\n",
    "        labels=seq.labels,\n",
    "        annot_path=a_timit.annot_path\n",
    "    )\n",
    "    return a_simpleseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb42bea-0f8d-469f-bcc5-a033c69414bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_frame_features(audio_path, hop_length_s=0.001, n_fft_s=0.025, n_mels=40, n_mfcc=13):\n",
    "    \"\"\"Converts audio into framewise features\n",
    "\n",
    "    Adapted from https://github.com/felixkreuk/SegFeat\n",
    "\n",
    "    > ... we extracted 13 Mel-Frequency Cepstrum Coefficients (MFCCs), \n",
    "    with delta and delta-delta features every 10 ms, \n",
    "    with a processing window size of 10ms.\n",
    "\n",
    "    Note this funciton uses a hop length of 1 ms, not 10 ms.\n",
    "    \n",
    "    > Moreover, we concatenated four additional features based\n",
    "    on the spectral changes between adjacent frames, using\n",
    "    MFCCs to represent the spectral properties of the frames.\n",
    "    Define Dt,j = d(at−j , at+j ) to be the Euclidean distance\n",
    "    between the MFCC feature vectors at−j and at+j , where\n",
    "    at ∈ R39 for 1 ≤ t ≤ T . The features are denoted by Dt,j ,\n",
    "    for j ∈ {1, 2, 3, 4}. We observed this set of features greatly\n",
    "    improves performance over the standard MFCC features.\n",
    "\n",
    "    See also: https://groups.google.com/g/librosa/c/V4Z1HpTKn8Q/m/1-sMpjxjCSoJ\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    hop_length = int(hop_length_s * sr)\n",
    "    n_fft = int(n_fft_s * sr)\n",
    "    spect = librosa.feature.mfcc(y=y,\n",
    "                                 sr=sr,\n",
    "                                 n_fft=n_fft,\n",
    "                                 hop_length=hop_length,\n",
    "                                 n_mels=n_mels,\n",
    "                                 n_mfcc=n_mfcc\n",
    "                                )\n",
    "\n",
    "    delta  = librosa.feature.delta(spect, order=1)\n",
    "    delta2 = librosa.feature.delta(spect, order=2)\n",
    "    spect  = np.concatenate([spect, delta, delta2], axis=0)\n",
    "    dist = []\n",
    "    for i in range(2, 9, 2):\n",
    "        pad = int(i/2)\n",
    "        d_i = np.concatenate([np.zeros(pad), ((spect[:, i:] - spect[:, :-i]) ** 2).sum(0) ** 0.5, np.zeros(pad)], axis=0)\n",
    "        dist.append(d_i)\n",
    "    dist = np.stack(dist)\n",
    "    frames = np.concatenate([spect, dist], axis=0)\n",
    "    times = librosa.frames_to_time(\n",
    "        np.arange(frames.shape[1]), sr=sr, hop_length=hop_length, n_fft=n_fft\n",
    "    )\n",
    "    return frames, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e853b7-876e-4c92-b6a3-86068568eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMIT_ROOT = pathlib.Path(\"./data/raw/timit.deepai/\")\n",
    "TIMIT_DATA_ROOT = TIMIT_ROOT / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f576b75-302c-40c1-b012-e4b3a7956e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_paths = sorted((TIMIT_DATA_ROOT / \"TRAIN\").glob(\"**/*PHN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d96b3fd-c0c4-40ed-a3f3-2288c85aaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_files_for_split(split_name, dst):\n",
    "    annot_paths = sorted((TIMIT_DATA_ROOT / split_name.upper()).glob(\"**/*PHN\"))\n",
    "    audio_paths = sorted((TIMIT_DATA_ROOT / split_name.upper()).glob(\"**/*WAV.wav\"))\n",
    "    assert len(annot_paths) == len(audio_paths)\n",
    "\n",
    "    records = []\n",
    "    timebin_dur = None\n",
    "    for audio_path, annot_path in tqdm(zip(audio_paths, annot_paths)):\n",
    "        # get strings for file names\n",
    "        dialect_region = annot_path.parents[1].name\n",
    "        speaker = annot_path.parents[0].name\n",
    "\n",
    "        frames, times = get_frame_features(audio_path)\n",
    "        if timebin_dur is None:\n",
    "            timebin_dur = np.diff(times).mean()\n",
    "        fname = f\"{dialect_region}-{speaker}-{audio_path.name}.spect.npz\"\n",
    "        frames_path = dst / fname\n",
    "        np.savez(frames_path, s=frames, t=times, f=np.arange(frames.shape[0]))\n",
    "        \n",
    "        a_simpleseq = simpleseq_from_timit_phn_path(annot_path)\n",
    "        simpleseq_path = dst / f\"{dialect_region}-{speaker}-{annot_path.stem}.csv\"\n",
    "        a_simpleseq.to_file(simpleseq_path)\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                'audio_path': audio_path,\n",
    "                \"spect_path\": frames_path,\n",
    "                \"annot_path\": simpleseq_path,\n",
    "                \"annot_format\": \"simple-seq\",\n",
    "                \"spect_dur\": frames.shape[1] * timebin_dur,\n",
    "                \"timebin_dur\": timebin_dur,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    source_paths_df = pd.DataFrame.from_records(records)\n",
    "    return source_paths_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4411e5e-0b58-4ad4-80b4-9efd432cc064",
   "metadata": {},
   "source": [
    "We set a destination (`DST`) for the files we will generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b61093e-a27b-409a-97c7-c61758d5b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DST = pathlib.Path(\"./data/spectrograms-annotations/timit.deepai\")\n",
    "DST.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a78566e-80df-4756-84dc-7a09eb86684b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7c4f1b63624018b0c9f3a623302b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb64bb4be90b401e8a4126da6462676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_files_df = []\n",
    "\n",
    "for split_name in ('train', 'test'):\n",
    "    source_paths_df = prep_files_for_split(split_name, DST)\n",
    "    source_paths_df['split'] = split_name\n",
    "    if split_name == 'train':\n",
    "        val_inds = np.random.randint(low=len(source_paths_df), size=int(len(source_paths_df) * 0.1))\n",
    "        source_paths_df.loc[val_inds, 'split'] = 'val'\n",
    "    source_files_df.append(source_paths_df)\n",
    "\n",
    "source_files_df = pd.concat(source_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df84d8c2-51e7-4837-9161-250a6d26f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files_csv_path = DST / f\"{DST.name}_prep_{vak.common.timenow.get_timenow_as_str()}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207ceeda-1e8a-4036-ba7d-1e8ce07ecbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files_df.to_csv(source_files_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c831f-dd0e-4c05-b9c6-bb1af15186d3",
   "metadata": {},
   "source": [
    "## 2. Make frame classification dataset from sourc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d8bfb2-5848-4ec9-824c-50be656c6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files_csv_path = sorted(DST.glob(f\"{DST.name}_prep_*.csv\"))\n",
    "assert len(source_files_csv_path) > 0, \"No source files csv paths\"\n",
    "source_files_csv_path = source_files_csv_path[-1]  # use most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2401d61d-cd7f-4a4b-b7c4-1129b6e07088",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files_df = pd.read_csv(source_files_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a79f36b2-7e85-4f86-b143-2be327195764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "labelmap_json_path = dataset_path / \"labelmap.json\"\n",
    "\n",
    "\n",
    "if not labelmap_json_path.exists():\n",
    "    SIMPLESEQ_SCRIBE = crowsetta.Transcriber(format='simple-seq')\n",
    "    annot_paths = source_files_df['annot_path'].values\n",
    "    labelset = set(\n",
    "        [lbl \n",
    "         for annot_path in annot_paths \n",
    "         for lbl in SIMPLESEQ_SCRIBE.from_file(annot_path).to_seq().labels\n",
    "        ]\n",
    "    )\n",
    "    labelmap = vak.common.labels.to_map(labelset, map_unlabeled=True)\n",
    "    with labelmap_json_path.open(\"w\") as fp:\n",
    "        json.dump(labelmap, fp)\n",
    "else:\n",
    "    with labelmap_json_path.open(\"r\") as fp:\n",
    "        labelmap = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bc76c58-8cb0-43ea-9c88-05144a9f4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = pathlib.Path(\n",
    "    \"./data/prep/multiclass/timit.deepai/generated_20240323\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78ddda5b-29d6-4658-9d18-7fb0bf48dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a50e3a-6882-4228-9f17-68359d6f5052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 16.44 s\n",
      "[########################################] | 100% Completed | 18.61 s\n",
      "[########################################] | 100% Completed | 15.26 s\n"
     ]
    }
   ],
   "source": [
    "dataset_df = vak.prep.frame_classification.make_splits.make_splits(\n",
    "    dataset_df=source_files_df,\n",
    "    dataset_path=dataset_path,\n",
    "    input_type='spect',\n",
    "    purpose='train',\n",
    "    labelmap=labelmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deca19b5-4fef-4ef1-806e-2b18cf7d23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_csv_path = vak.prep.dataset_df_helper.get_dataset_csv_path(\n",
    "    dataset_path, 'timit.deepai', '2024-03-23'\n",
    ")\n",
    "dataset_df.to_csv(\n",
    "    dataset_csv_path, index=False\n",
    ")  # index is False to avoid having \"Unnamed: 0\" column when loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aced9dec-d7ee-4db4-b1ae-0400aab851ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_dur = vak.prep.frame_classification.validators.validate_and_get_frame_dur(\n",
    "    dataset_df, \"spect\"\n",
    ")\n",
    "\n",
    "metadata = vak.datasets.frame_classification.Metadata(\n",
    "    dataset_csv_filename=str(dataset_csv_path.name),\n",
    "    frame_dur=frame_dur,\n",
    "    input_type=\"spect\",\n",
    "    audio_format=\"wav\",\n",
    "    spect_format=\"npz\",\n",
    ")\n",
    "metadata.to_json(dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
